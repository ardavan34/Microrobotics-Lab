Parameters:
    hyperparam = {'neuralNet': ArdavanNet_2(), 'modelName': "ArdavanNet_2", 'batchSize': 256,
                  'learning rate': 1e-2, 'lossFunction': torch.nn.MSELoss(), 'epochsNum': 1000}
    hyperparam['optimizer'] = torch.optim.Adam(model.parameters(), lr=hyperparam['learning rate'])
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer=hyperparam['optimizer'], step_size=1, gamma=0.999, last_epoch=-1)


Network Architecture:
    ArdavanNet_2(
        (recurrentLayer): Sequential(
            (0): Linear(in_features=11, out_features=128, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.4, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): ReLU()
            (5): Linear(in_features=128, out_features=11, bias=True)
        )
        (finalLayer): Sequential(
            (0): Linear(in_features=11, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=32, bias=True)
            (3): ReLU()
            (4): Dropout(p=0.3, inplace=False)
            (5): Linear(in_features=32, out_features=8, bias=True)
            (6): ReLU()
            (7): Linear(in_features=8, out_features=3, bias=True)
        )
    )


Forward:
    def block(self, input):
        x1 = self.recurrentLayer(input)
        x2 = self.recurrentLayer(x1)

        return x2 + input

    def forward(self, input):
        """
        Apply the forward propagation
        """
        logits = input
        for i in range(3):
            logits = SimpleNeuralNetwork.block(self, logits)

        logits = self.finalLayer(logits)

        return logits