Parameters:
    hyperparam = {'neuralNet': SimpleNeuralNetwork(), 'modelName': "ArdavanNet_2", 'batchSize': 256,
                  'learning rate': 1e-3, 'lossFunction': torch.nn.MSELoss(), 'epochsNum': 800}
    hyperparam['optimizer'] = torch.optim.Adam(model.parameters(), lr=hyperparam['learning rate'], weight_decay=1e-3)


Network Architecture:
    SimpleNeuralNetwork(
        (layerBlock): Sequential(
            (0): Linear(in_features=11, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=64, bias=True)
            (3): Dropout(p=0.2, inplace=False)
            (4): ReLU()
            (5): Linear(in_features=64, out_features=32, bias=True)
            (6): ReLU()
            (7): Linear(in_features=32, out_features=11, bias=True)
            (8): Dropout(p=0.2, inplace=False)
            (9): ReLU()
        )
        (endingBlock): Sequential(
            (0): Linear(in_features=11, out_features=32, bias=True)
            (1): ReLU()
            (2): Linear(in_features=32, out_features=16, bias=True)
            (3): Dropout(p=0.2, inplace=False)
            (4): ReLU()
            (5): Linear(in_features=16, out_features=3, bias=True)
        )
    )


Forward:
    x1 = self.layerBlock(input)
    x2 = self.layerBlock(x1) + x1
    x3 = self.layerBlock(x2) + x1 + x2
    x4 = self.layerBlock(x3) + x1 + x2 + x3
    logits = self.endingBlock(x4)
    return logits